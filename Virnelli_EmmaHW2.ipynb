{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1811538",
   "metadata": {},
   "source": [
    "# Homework 2\n",
    "\n",
    "### Name: Emma Virnelli\n",
    "### Collaborator:\n",
    "\n",
    "\n",
    "\n",
    "DATA 201\n",
    "\n",
    "Fall 2024\n",
    "\n",
    "Tufts University"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06134ec",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f9de6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a876f9",
   "metadata": {},
   "source": [
    "## 1. Numpy Array Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b0efc1",
   "metadata": {},
   "source": [
    "### Objective:\n",
    "\n",
    "#### - To master the creation and manipulation of Numpy arrays and understand array operations.\n",
    "\n",
    "### Instructions:\n",
    "\n",
    "#### 1. Create Arrays:\n",
    "#### -Create two Numpy arrays with 5 elements each.\n",
    "#### 2. Perform Operations:\n",
    "#### -Perform element-wise addition, subtraction, multiplication, and division on the created arrays.\n",
    "#### 3. Print Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a5c9601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Addition: [ 7 10 12 13 15]\n",
      "Subtraction: [3 2 2 3 3]\n",
      "Multiplication: [10 24 35 40 54]\n",
      "Division: [2.5 1.5 1.4 1.6 1.5]\n"
     ]
    }
   ],
   "source": [
    "#Code\n",
    "\n",
    "#Create Arrays\n",
    "x = np.array([2, 4, 5, 5, 6])\n",
    "y = np.array([5, 6, 7, 8, 9])\n",
    "\n",
    "#Perform Operations\n",
    "sum = y + x\n",
    "sub = y - x\n",
    "mult = y * x\n",
    "divide = y / x\n",
    "\n",
    "#Print Result\n",
    "print(\"Addition:\", sum)\n",
    "print(\"Subtraction:\", sub)\n",
    "print(\"Multiplication:\", mult)\n",
    "print(\"Division:\", divide)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628978de",
   "metadata": {},
   "source": [
    "## 2. Pandas Data Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b658a8c6",
   "metadata": {},
   "source": [
    "### Objective:\n",
    "\n",
    "#### - To refine skills in manipulating DataFrame objects, handling indices, and applying functions to columns in Pandas.\n",
    "\n",
    "### Instructions:\n",
    "\n",
    "#### 1. Load Dataset:\n",
    "#### - Load the Iris dataset into a DataFrame. (https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data)\n",
    "#### - The column names are 'sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species'\n",
    "#### 2. Apply Functions:\n",
    "#### - Create a new column, sepal_ratio, by applying a lambda function that divides sepal_length by sepal_width.\n",
    "#### 3. Print Result (.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "955f1b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sepal_length  sepal_width  petal_length  petal_width      species  \\\n",
      "0           5.1          3.5           1.4          0.2  Iris-setosa   \n",
      "1           4.9          3.0           1.4          0.2  Iris-setosa   \n",
      "2           4.7          3.2           1.3          0.2  Iris-setosa   \n",
      "3           4.6          3.1           1.5          0.2  Iris-setosa   \n",
      "4           5.0          3.6           1.4          0.2  Iris-setosa   \n",
      "\n",
      "   sepal_ratio  \n",
      "0     1.457143  \n",
      "1     1.633333  \n",
      "2     1.468750  \n",
      "3     1.483871  \n",
      "4     1.388889  \n"
     ]
    }
   ],
   "source": [
    "#Code\n",
    "import pandas as pd\n",
    "\n",
    "#Load Dataset\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\n",
    "columns=(['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species'])\n",
    "iris = pd.read_csv(url, names=columns)\n",
    "\n",
    "#Apply Functions\n",
    "iris['sepal_ratio'] = iris.apply(lambda x: x['sepal_length'] / x['sepal_width'], axis = 1)\n",
    "\n",
    "#Print Result\n",
    "print(iris.head(5))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8088ec20",
   "metadata": {},
   "source": [
    "## 3.  Advanced Data Aggregation with Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a17336e",
   "metadata": {},
   "source": [
    "### Objective:\n",
    "\n",
    "#### - To gain proficiency in using advanced data aggregation methods in Pandas, such as groupby and pivot_table.\n",
    "\n",
    "### Instructions:\n",
    "\n",
    "#### 1. Load Dataset:\n",
    "#### - Load the Gapminder dataset. (https://raw.githubusercontent.com/resbaz/r-novice-gapminder-files/master/data/gapminder-FiveYearData.csv)\n",
    "#### 2. Groupby Operations:\n",
    "#### - Use groupby to find the average life expectancy and GDP per capita for each continent for the year 2007.\n",
    "#### 3. Advanced Pivot Table:\n",
    "#### - Create a pivot table with continent as an index, year as columns, and fill values with the mean of gdpPercap.\n",
    "#### 4. Print Results (.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe18dae5-4675-42a2-9d21-21494fd03daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouped Data:\n",
      "  continent  avg_lifeExp  avg_gdpPercap\n",
      "0    Africa    54.806038    3089.032605\n",
      "1  Americas    73.608120   11003.031625\n",
      "2      Asia    70.728485   12473.026870\n",
      "3    Europe    77.648600   25054.481636\n",
      "4   Oceania    80.719500   29810.188275\n"
     ]
    }
   ],
   "source": [
    "#Code\n",
    "import pandas as pd\n",
    "\n",
    "#Groupby Operations\n",
    "gap_dataset = pd.read_csv('https://raw.githubusercontent.com/resbaz/r-novice-gapminder-files/master/data/gapminder-FiveYearData.csv')\n",
    "gap_2007 = gap_dataset[gap_dataset['year'] == 2007]\n",
    "grouped_gap = gap_2007.groupby('continent').agg(avg_lifeExp=('lifeExp', 'mean'), avg_gdpPercap=('gdpPercap', 'mean')).reset_index()\n",
    "\n",
    "print(\"Grouped Data:\")\n",
    "print(grouped_gap.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb8ea350-47f8-42df-b621-d94140ad3561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pivot Table:\n",
      "year               1952          1957          1962          1967  \\\n",
      "continent                                                           \n",
      "Africa      1252.572466   1385.236062   1598.078825   2050.363801   \n",
      "Americas    4079.062552   4616.043733   4901.541870   5668.253496   \n",
      "Asia        5195.484004   5787.732940   5729.369625   5971.173374   \n",
      "Europe      5661.057435   6963.012816   8365.486814  10143.823757   \n",
      "Oceania    10298.085650  11598.522455  12696.452430  14495.021790   \n",
      "\n",
      "year               1972          1977          1982          1987  \\\n",
      "continent                                                           \n",
      "Africa      2339.615674   2585.938508   2481.592960   2282.668991   \n",
      "Americas    6491.334139   7352.007126   7506.737088   7793.400261   \n",
      "Asia        8187.468699   7791.314020   7434.135157   7608.226508   \n",
      "Europe     12479.575246  14283.979110  15617.896551  17214.310727   \n",
      "Oceania    16417.333380  17283.957605  18554.709840  20448.040160   \n",
      "\n",
      "year               1992          1997          2002          2007  \n",
      "continent                                                          \n",
      "Africa      2281.810333   2378.759555   2599.385159   3089.032605  \n",
      "Americas    8044.934406   8889.300863   9287.677107  11003.031625  \n",
      "Asia        8639.690248   9834.093295  10174.090397  12473.026870  \n",
      "Europe     17061.568084  19076.781802  21711.732422  25054.481636  \n",
      "Oceania    20894.045885  24024.175170  26938.778040  29810.188275  \n"
     ]
    }
   ],
   "source": [
    "# Advanced Pivot Table:\n",
    "pivot_table_gdp = gap_dataset.pivot_table(index='continent',columns='year',values='gdpPercap',aggfunc='mean')\n",
    "\n",
    "#Print Results \n",
    "print(\"Pivot Table:\")\n",
    "print(pivot_table_gdp.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee194d7",
   "metadata": {},
   "source": [
    "## 5. Titanic "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31f818a",
   "metadata": {},
   "source": [
    "### Objective:\n",
    "\n",
    "#### - To understand advanced grouping, filtering and aggregation methods using Pandas.\n",
    "\n",
    "#### Instructions:\n",
    "\n",
    "#### 1. Load the Dataset:\n",
    "#### - Load the Titanic dataset. (https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv)\n",
    "#### 2. Group by Class and Gender:\n",
    "#### - Group the Titanic dataset by Pclass and Sex and compute the average age within each group.\n",
    "#### 3. Filtering Using Lambda:\n",
    "#### - Filter the dataset to include only those rows where the number of Siblings/Spouses Aboard is greater than the number of Parents/Children Aboard.\n",
    "#### 4. Print Results (.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "97ab3f82-69f6-4604-bcbf-7ce010685dfe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Averaged Data:\n",
      "                     Age\n",
      "Pclass Sex              \n",
      "1      female  35.255319\n",
      "       male    41.511639\n",
      "2      female  28.980263\n",
      "       male    30.493796\n",
      "3      female  22.135417\n",
      "       male    26.470612\n"
     ]
    }
   ],
   "source": [
    "#Code\n",
    "\n",
    "#Load the Dataset\n",
    "titanic_dataset = pd.read_csv('https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv')\n",
    "\n",
    "#Group by Class and Gender\n",
    "averaged_data = titanic_dataset.groupby(['Pclass', 'Sex'])[['Age']].mean()\n",
    "\n",
    "print(\"Averaged Data:\")\n",
    "print(averaged_data) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9cd32ace-ccd7-4327-9b37-a399adac4611",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Data:\n",
      "   Survived  Pclass                                               Name  \\\n",
      "0         0       3                             Mr. Owen Harris Braund   \n",
      "1         1       1  Mrs. John Bradley (Florence Briggs Thayer) Cum...   \n",
      "3         1       1        Mrs. Jacques Heath (Lily May Peel) Futrelle   \n",
      "7         0       3                      Master. Gosta Leonard Palsson   \n",
      "9         1       2                 Mrs. Nicholas (Adele Achem) Nasser   \n",
      "\n",
      "      Sex   Age  Siblings/Spouses Aboard  Parents/Children Aboard     Fare  \n",
      "0    male  22.0                        1                        0   7.2500  \n",
      "1  female  38.0                        1                        0  71.2833  \n",
      "3  female  35.0                        1                        0  53.1000  \n",
      "7    male   2.0                        3                        1  21.0750  \n",
      "9  female  14.0                        1                        0  30.0708  \n"
     ]
    }
   ],
   "source": [
    "#Filtering Using Lambda\n",
    "filter = lambda x, y: x > y\n",
    "df = titanic_dataset[filter(titanic_dataset['Siblings/Spouses Aboard'], titanic_dataset['Parents/Children Aboard'])]\n",
    "\n",
    "#Print Results\n",
    "print('Filtered Data:')\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a609ff",
   "metadata": {},
   "source": [
    "## 6. Comprehensive Data Analysis with Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e854833d",
   "metadata": {},
   "source": [
    "### Objective:\n",
    "\n",
    "#### - To synthesize knowledge of Pandas, Numpy, data cleaning, manipulation, and aggregation to perform comprehensive analysis on a real-world dataset.\n",
    "\n",
    "### Instructions:\n",
    "\n",
    "#### 1. Load the Auto MPG Dataset (https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data-original):\n",
    "#### - Download the Auto MPG dataset and load it into a DataFrame.\n",
    "#### - Use appropriate column names based on the dataset documentation.\n",
    "#### 2. Data Cleaning:\n",
    "#### - Identify and drop any missing or anomalous values.\n",
    "#### 3. Data Transformation and Feature Engineering:\n",
    "#### - Compute the Displacement-to-Weight ratio and add it as a new feature in the DataFrame.\n",
    "#### - Extract the car Brand from the car name column and create a new column, Brand.\n",
    "#### 4. Data Aggregation:\n",
    "#### - Group the dataset by Brand and model year and calculate the average mpg for each group.\n",
    "#### - Create a pivot table to illustrate the average mpg of cars for each combination of Brand and model year.\n",
    "#### 5. Advanced Filtering and Sorting:\n",
    "#### - Identify the top 5 cars with the highest mpg within each model year.\n",
    "#### - Create a new DataFrame containing only these top 5 cars per model year.\n",
    "#### 6. Print Results (.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1411a180-4210-4133-882e-d20b8bfccb95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Data:\n",
      "    MPG  Cylinders  Displacement  Horsepower  Weight  Acceleration  \\\n",
      "0  18.0        8.0         307.0       130.0  3504.0          12.0   \n",
      "1  15.0        8.0         350.0       165.0  3693.0          11.5   \n",
      "2  18.0        8.0         318.0       150.0  3436.0          11.0   \n",
      "3  16.0        8.0         304.0       150.0  3433.0          12.0   \n",
      "4  17.0        8.0         302.0       140.0  3449.0          10.5   \n",
      "\n",
      "   Model Year  Origin                   Car Name  \n",
      "0        70.0     1.0  chevrolet chevelle malibu  \n",
      "1        70.0     1.0          buick skylark 320  \n",
      "2        70.0     1.0         plymouth satellite  \n",
      "3        70.0     1.0              amc rebel sst  \n",
      "4        70.0     1.0                ford torino  \n",
      "Pivot Table:\n",
      "Model Year  70.0       71.0  72.0   73.0       74.0  75.0  76.0   77.0   78.0  \\\n",
      "Brand                                                                           \n",
      "amc         17.5  18.333333  16.0  15.75  16.333333  18.0  18.5    NaN  18.75   \n",
      "audi        24.0        NaN   NaN  20.00  29.000000  23.0   NaN    NaN  20.30   \n",
      "bmw         26.0        NaN   NaN    NaN        NaN   NaN   NaN  21.50    NaN   \n",
      "buick       14.5        NaN  13.0  12.50  13.000000  19.0   NaN  25.25  19.15   \n",
      "cadillac     NaN        NaN   NaN    NaN        NaN   NaN  16.5    NaN    NaN   \n",
      "\n",
      "Model Year   79.0   80.0  81.0  82.0  \n",
      "Brand                                 \n",
      "amc         23.80  24.30   NaN   NaN  \n",
      "audi          NaN  35.35   NaN   NaN  \n",
      "bmw           NaN    NaN   NaN   NaN  \n",
      "buick       22.65    NaN  24.5  25.0  \n",
      "cadillac    23.00    NaN   NaN   NaN  \n",
      "Top 5 Cars per Year:\n",
      "   Model Year       Brand   MPG\n",
      "0        70.0      datsun  27.0\n",
      "1        70.0         bmw  26.0\n",
      "2        70.0  volkswagen  26.0\n",
      "3        70.0     peugeot  25.0\n",
      "4        70.0        saab  25.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h1/7lhgghr54qngtss004x01t3w0000gn/T/ipykernel_98930/3360963333.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cleaned_dataset.loc[:,'Displacement-to-Weight ratio'] = cleaned_dataset['Displacement'] / cleaned_dataset['Weight']\n",
      "/var/folders/h1/7lhgghr54qngtss004x01t3w0000gn/T/ipykernel_98930/3360963333.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cleaned_dataset.loc[:,'Brand'] = cleaned_dataset['Car Name'].str.split().str[0]\n",
      "/var/folders/h1/7lhgghr54qngtss004x01t3w0000gn/T/ipykernel_98930/3360963333.py:28: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top_cars = brand_year.groupby('Model Year').apply(lambda x: x.nlargest(5, 'MPG')).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "#Code\n",
    "\n",
    "#Load the Auto MPG Dataset\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data-original'\n",
    "columns = ['MPG', 'Cylinders', 'Displacement', 'Horsepower', 'Weight', 'Acceleration', 'Model Year', 'Origin', 'Car Name']\n",
    "auto_dataset = pd.read_csv(url, names=columns, sep = r'\\s+', na_values='?')\n",
    "\n",
    "#Data Cleaning\n",
    "cleaned_dataset = auto_dataset.dropna()  \n",
    "\n",
    "print('Cleaned Data:')\n",
    "print(cleaned_dataset.head())\n",
    "\n",
    "#Data Transformation\n",
    "cleaned_dataset.loc[:,'Displacement-to-Weight ratio'] = cleaned_dataset['Displacement'] / cleaned_dataset['Weight']\n",
    "cleaned_dataset.loc[:,'Brand'] = cleaned_dataset['Car Name'].str.split().str[0]\n",
    "\n",
    "#Data Aggregation\n",
    "brand_year = cleaned_dataset.groupby(['Model Year', 'Brand'])['MPG'].mean().reset_index()\n",
    "pivot_table = brand_year.pivot_table(index='Brand', columns='Model Year', values='MPG', aggfunc='mean')\n",
    "\n",
    "print(\"Pivot Table:\")\n",
    "print(pivot_table.head())\n",
    "\n",
    "#Advanced Filtering and Sorting\n",
    "top_cars = brand_year.groupby('Model Year').apply(lambda x: x.nlargest(5, 'MPG')).reset_index(drop=True)\n",
    "\n",
    "\n",
    "print(\"Top 5 Cars per Year:\")\n",
    "print(top_cars.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
