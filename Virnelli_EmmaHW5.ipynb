{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 5\n",
    "\n",
    "### Name: Emma Virnelli\n",
    "### Collaborator: Tomiwa\n",
    "\n",
    "\n",
    "DATA 201\n",
    "\n",
    "Fall 2024\n",
    "\n",
    "Tufts University"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This homework explores KNN, Decision Trees, and Random Forests. The first question reviews training a KNN model. Subsequent questions provide an in-depth examination of Gini impurity and the mechanics of training a Decision Tree. Following this, we delve into basic implementations of both Decision Trees and Random Forests, accompanied by an introduction to tuning hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "\n",
    "(a) Load the Heart Disease dataset and name it 'df'. Conduct data cleaning.\n",
    "\n",
    "- Perform any data cleaning or data transformation steps if required\n",
    "- Explain some of the data cleaning steps which you can perform on **any** data set\n",
    "\n",
    "For clarification, please find the metadata below:\n",
    "- BPMeds: whether or not the patient was on blood pressure medication\n",
    "- prevalentStroke: whether or not the patient had previously had a stroke\n",
    "- prevalentHyp: whether or not the patient was hypertensive\n",
    "- diabetes: whether or not the patient had diabetes\n",
    "- totChol: total cholesterol level\n",
    "- sysBP: systolic blood pressure\n",
    "- diaBP: diastolic blood pressure\n",
    "- BMI: Body Mass Index\n",
    "- heartRate: heart rate\n",
    "- glucose: glucose level\n",
    "- 10 year risk of coronary heart disease CHD (binary: “1”, means “Yes”, “0” means “No”) (Predictor Variable)\n",
    "\n",
    "(b) Create two dataframes for features and target variable (TenYearCHD). Conduct a full model training and testing (30%) for a KNN model with K=5. Print the accuracy, confusion matrix, and explain what the confuison matrix tells you\n",
    "\n",
    "(c) Create a range from 1 to 50 going at steps of 2 then make a list where you will store average accuracy at each k value\n",
    "use a for loop to compute the average accuracy over 10-fold cross validation for each k value. Plot the average accuracy for each k values and determine which k value you will choose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1 (a)\n",
    "\n",
    "# Load the Heart Disease dataset\n",
    "df = pd.read_csv(\"Heart Disease.csv\")\n",
    "\n",
    "\n",
    "# Check is there is any null values and drop null values in order to clean it \n",
    "df.isnull().sum()\n",
    "df = df.dropna() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8279\n",
      "Confusion Matrix:\n",
      "[[893  29]\n",
      " [160  16]]\n"
     ]
    }
   ],
   "source": [
    "#Q1 (b)\n",
    "\n",
    "# Create two dataframes for features and target variable (TenYearCHD)\n",
    "X = df.drop('TenYearCHD', axis=1)\n",
    "y = df['TenYearCHD']\n",
    "\n",
    "# Full model training and testing (30%) \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# KNN model with K=5\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print the accuracy, confusion matrix, and explain what the confuison matrix tells you\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix is explaining the number of times the models classified the classes correctly or incorrectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best K value: 15\n"
     ]
    }
   ],
   "source": [
    "#Q1 (c)\n",
    "\n",
    "# Create a range from 1 to 50 going at steps of 2\n",
    "k_range = range(1, 51, 2)\n",
    "avg_accuracies = []\n",
    "\n",
    "\n",
    "# Store average accuracy at each k value use a for loop to compute the average accuracy over 10-fold cross validation for each k value\n",
    "for k in k_range:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn, X, y, cv=10, scoring='accuracy')\n",
    "    avg_accuracies.append(scores.mean())\n",
    "\n",
    "# Plot the average accuracy for each k values\n",
    "plt.plot(k_range, avg_accuracies)\n",
    "plt.xlabel('K value')\n",
    "plt.ylabel('Average Accuracy')\n",
    "plt.title('Average Accuracy vs K value')\n",
    "plt.show()\n",
    "\n",
    "# Determine which k value you will choose\n",
    "best_k = k_range[np.argmax(avg_accuracies)]\n",
    "print(f\"Best K value: {best_k}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "\n",
    "The goal of Q2 is to make a function that can calculate the weighted gini impurity over any grouping and any size of classes. \n",
    "\n",
    "(a) Calculate the gini impurity of the example by hand. Then write some code to do it for you. You will have to find $p_0$ and $p_1$ which are the probabilities of selecting a 0 and a 1 (respectively) from the group.\n",
    "\n",
    "(b) Now say we have two groups? Calculate the gini index of each group. Now, make a weighted sum of the gini numbers, each weighted by the proportion of the group size to the total number of entries (i.e. if group 1 is of length 10 and group 2 is of length 15, then group 1 would have a weight of 2/5 and group 2 a weight of 3/5).\n",
    "\n",
    "(c) Generalize that bit of code you wrote to now deal with any number of groups with any number of classes. Assume the groups will be given as a list of lists, and if there is an empty group the gini number will be 0 (there needs to be an if statement to make sure we do not divide by 0).\n",
    "\n",
    "(d) Make that bit of code into a function called gini_imp which takes a 'groups' variable and a 'classes' variable. It should return the gini index. Note, you need to use the classes variable because that is how you know what classes to check for in each group. Test it on the same example from part (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gini Impurity: 0.444\n"
     ]
    }
   ],
   "source": [
    "#Q2 (a)\n",
    "\n",
    "###This is the example\n",
    "classes = [0,1]\n",
    "group = [0,0,0,1,1,0,1,0,0]\n",
    "\n",
    "class_value_counts = pd.Series(group).value_counts()\n",
    "\n",
    "# Gini Impurity Code\n",
    "def gini_impurity(value_counts):\n",
    "    n = value_counts.sum()\n",
    "    p_sum = 0\n",
    "    for key in value_counts.keys():\n",
    "        p_sum = p_sum + (value_counts[key] / n) * (value_counts[key] / n)\n",
    "    gini = 1 - p_sum\n",
    "    return gini\n",
    "\n",
    "\n",
    "gini = gini_impurity(class_value_counts)\n",
    "print(f\"Gini Impurity: {gini:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Gini Impurity: 0.438\n"
     ]
    }
   ],
   "source": [
    "#Q2 (b)\n",
    "classes = [0,1]\n",
    "groups = [[0,0,0,1],[0,0,1,1]]\n",
    "\n",
    "# Since I created the function to calculate the gini impurity, I will use it again\n",
    "def gini_impurity(value_counts):\n",
    "    n = value_counts.sum()\n",
    "    p_sum = 0\n",
    "    for key in value_counts.keys():\n",
    "        p_sum = p_sum + (value_counts[key] / n) * (value_counts[key] / n)\n",
    "    gini = 1 - p_sum\n",
    "    return gini\n",
    "\n",
    "#Take groups and classify seperately\n",
    "group_one = [0,0,0,1]\n",
    "group_two = [0,0,1,1]\n",
    "\n",
    "class_value_counts1 = pd.Series(group_one).value_counts()\n",
    "class_value_counts2 = pd.Series(group_two).value_counts()\n",
    "\n",
    "gini_one = gini_impurity(class_value_counts1)\n",
    "gini_two = gini_impurity(class_value_counts2)\n",
    "\n",
    "# Total length and weights\n",
    "total_length = len(group_one) + len(group_two)\n",
    "weight_one = len(group_one) / total_length\n",
    "weight_two = len(group_two) / total_length\n",
    "\n",
    "# Weighted sum of the gini numbers\n",
    "weighted_gini = (weight_one * gini_one) + (weight_two * gini_two)\n",
    "\n",
    "\n",
    "print(f\"Weighted Gini Impurity: {weighted_gini:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generalized Gini Impurity: 0.5684\n"
     ]
    }
   ],
   "source": [
    "#Q2 (c)\n",
    "###test your bit of code with:\n",
    "classes = [0,1,2,3]\n",
    "groups = [[0,3,1,1,1],[0,0,0,1],[2,3,1,1],[],[0,0,1,1,2,2]]\n",
    "\n",
    "\n",
    "def gini_impurity(value_counts):\n",
    "    n = sum(value_counts.values())  \n",
    "    if n == 0: \n",
    "        return 0\n",
    "    p_sum = 0\n",
    "    for count in value_counts.values():\n",
    "        p_sum += (count / n) ** 2\n",
    "    gini = 1 - p_sum\n",
    "    return gini\n",
    "\n",
    "# Generalize that bit of code you wrote to now deal with any number of groups with any number of classes\n",
    "def generalized_gini(groups, classes):\n",
    "    total_instances = sum(len(group) for group in groups)  \n",
    "    gini_total = 0.0\n",
    "    \n",
    "    for group in groups:\n",
    "        if len(group) == 0:\n",
    "            continue  # needs to be an if statement to make sure we do not divide by 0\n",
    "        value_counts = {cls: group.count(cls) for cls in classes}  \n",
    "        group_size = len(group)\n",
    "        gini = gini_impurity(value_counts)\n",
    "        gini_total += (group_size / total_instances) * gini  \n",
    "    \n",
    "    return gini_total\n",
    "\n",
    "\n",
    "result = generalized_gini(groups, classes)\n",
    "print(f\"Generalized Gini Impurity: {result:.4f}\")\n",
    "\n",
    "\n",
    "###You should get a gini number equal to 0.5684"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gini Impurity: 0.5684\n"
     ]
    }
   ],
   "source": [
    "#Q2 (d)\n",
    "\n",
    "# gini_imp which takes a 'groups' variable and a 'classes' variable\n",
    "def gini_imp(groups, classes):\n",
    "    total_samples = sum(len(group) for group in groups)\n",
    "    gini = 0.0\n",
    "    \n",
    "    for group in groups:\n",
    "        size = float(len(group))\n",
    "        if size == 0:\n",
    "            continue\n",
    "        \n",
    "        score = 0.0\n",
    "        for class_val in classes:\n",
    "            p = group.count(class_val) / size\n",
    "            score += p * p\n",
    "        \n",
    "        gini += (1.0 - score) * (size / total_samples)\n",
    "    \n",
    "    return gini\n",
    "\n",
    "# Test with example from part c\n",
    "classes = [0,1,2,3]\n",
    "groups = [[0,3,1,1,1],[0,0,0,1],[2,3,1,1],[],[0,0,1,1,2,2]]\n",
    "\n",
    "result = gini_imp(groups, classes)\n",
    "print(f\"Gini Impurity: {result:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "\n",
    "To make a full decision trees\n",
    "\n",
    "(a) Load the datasets titanic_train_data and titanic_test_data into dataframes and split them into X_train, X_test, y_train and y_test (the training sets are coming from the titanic train data and visa versa). Note, the first two columns should be deleted because they are just passenger Id's, also documentation about the data can be found here:https://www.kaggle.com/azeembootwala/titanic. Create a Decision Tree instance, fit the data on the training sets and get the accuracy score on the test set. Report the confusion matrix.\n",
    "\n",
    "(b) Split the X_train and y_train data into sets called X_train,X_val, y_train, y_val using train_test_split. We are going to try and tune some parameters in the Decision tree. Called a new Decision Tree instance with the following parameters: **max_features, max_depth, and min_samples_leaf**. Choose some value to test them with, and run the model a few times to see if you can get different accuracy scores (use the X_val and y_val to get the accuracy scores). What do these parameters do? (Look them up in the documentation)\n",
    "\n",
    "(c) We are going to use the validation sets to try and find the best parameter combinations. So, use a triple for loop to iterate over different ranges for each of the three parameters, find what combination gives the best accuracy on the validation set. Then, use that combination on a decision tree to classify the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Result:\n",
      "\n",
      "accuracy score: 0.8\n",
      "\n",
      "Confusion Matrix: \n",
      " [[52 12]\n",
      " [ 8 28]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Q3 (a)\n",
    "#Libraries for decision tree and random forest model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the datasets titanic_train_data and titanic_test_data into dataframes \n",
    "titanic_test_data  = pd.read_csv(\"titanic_test_data.csv\")\n",
    "titanic_train_data = pd.read_csv(\"titanic_train_data.csv\")\n",
    "\n",
    "#Delete first two columns for each training and test data\n",
    "X_train = titanic_train_data.drop(['Unnamed: 0', 'PassengerId', 'Survived'], axis=1)\n",
    "y_train = titanic_train_data['Survived']\n",
    "\n",
    "X_test = titanic_test_data.drop(['Unnamed: 0', 'PassengerId', 'Survived'], axis=1)\n",
    "y_test = titanic_test_data['Survived']\n",
    "\n",
    "tree_clf = DecisionTreeClassifier(random_state=42)\n",
    "tree_clf.fit(X_train, y_train)\n",
    "\n",
    "def print_score(clf, X_train, y_train, X_test, y_test, train=True, pos_label=\"Yes\"):\n",
    "    if train == False:\n",
    "        pred = clf.predict(X_test)\n",
    "        print(\"Test Result:\\n\")        \n",
    "        print(f\"accuracy score: {accuracy_score(y_test, pred)}\\n\")\n",
    "        print(f\"Confusion Matrix: \\n {confusion_matrix(y_test, pred)}\\n\")\n",
    "\n",
    "print_score(tree_clf, X_train, y_train, X_test, y_test, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_features=log2, max_depth=5, min_samples_leaf=5 \n",
      " Validation Accuracy: 0.8406\n",
      "\n",
      "max_features=7, max_depth=10, min_samples_leaf=3 \n",
      " Validation Accuracy: 0.8160\n",
      "\n",
      "max_features=sqrt, max_depth=15, min_samples_leaf=2 \n",
      " Validation Accuracy: 0.8005\n",
      "\n",
      "max_features=None, max_depth=8, min_samples_leaf=4 \n",
      " Validation Accuracy: 0.8133\n",
      "\n",
      "X_train features: ['male', 'age', 'education', 'currentSmoker', 'cigsPerDay', 'BPMeds', 'prevalentStroke', 'prevalentHyp', 'diabetes', 'totChol', 'sysBP', 'diaBP', 'BMI', 'heartRate', 'glucose']\n",
      "X_val features: ['male', 'age', 'education', 'currentSmoker', 'cigsPerDay', 'BPMeds', 'prevalentStroke', 'prevalentHyp', 'diabetes', 'totChol', 'sysBP', 'diaBP', 'BMI', 'heartRate', 'glucose']\n",
      "X_test features: ['Sex', 'Age', 'Fare', 'Pclass_1', 'Pclass_2', 'Pclass_3', 'Family_size', 'Title_1', 'Title_2', 'Title_3', 'Title_4', 'Emb_1', 'Emb_2', 'Emb_3']\n"
     ]
    }
   ],
   "source": [
    "#Q3 (b)\n",
    "\n",
    "# Split the X_train and y_train data into sets called X_train,X_val, y_train, y_val using train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Function to train and evaluate the Decision Tree\n",
    "def train_and_evaluate(max_features, max_depth, min_samples_leaf):\n",
    "    dt = DecisionTreeClassifier(\n",
    "        max_features=max_features,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        random_state=42\n",
    "    )\n",
    "    dt.fit(X_train, y_train)\n",
    "    y_pred = dt.predict(X_val)\n",
    "    return accuracy_score(y_val, y_pred)\n",
    "\n",
    "# Choose some value to test them with \n",
    "param_combinations = [\n",
    "    ('log2', 5, 5),\n",
    "    (7, 10, 3),\n",
    "    ('sqrt', 15, 2),\n",
    "    (None, 8, 4)\n",
    "]\n",
    "\n",
    "# Run the model a few times to see if you can get different accuracy scores (use the X_val and y_val to get the accuracy scores)\n",
    "for max_features, max_depth, min_samples_leaf in param_combinations:\n",
    "    accuracy = train_and_evaluate(max_features, max_depth, min_samples_leaf)\n",
    "    print(f\"max_features={max_features}, max_depth={max_depth}, min_samples_leaf={min_samples_leaf} \\n Validation Accuracy: {accuracy:.4f}\\n\")\n",
    "\n",
    "print(\"X_train features:\", X_train.columns.tolist())\n",
    "print(\"X_val features:\", X_val.columns.tolist())\n",
    "print(\"X_test features:\", X_test.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **max_features**: Specifies the number of features to consider when looking for the best split in a decision tree. It helps to reduce overfitting by limiting the amount of information each tree has access to, encouraging diversity among the trees in the forest.\n",
    "- **max_depth**: Limits the depth of the decision tree, preventing it from growing too deep and capturing noise in the data, which can lead to overfitting.\n",
    "- **min_samples_leaf**: Sets the minimum number of samples that a leaf node must have. Increasing this value ensures that the model doesn't create overly specific branches, helping to generalize better and avoid overfitting.\n",
    "\n",
    "Balancing these parameters is key: too many trees or too much depth can overfit, while too few may underfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'max_features': None, 'max_depth': 6, 'min_samples_leaf': 10}\n",
      "Best validation accuracy: 0.8583\n",
      "Test accuracy with best parameters: 0.8500\n",
      "Confusion Matrix for the final model:\n",
      "[[59  5]\n",
      " [10 26]]\n"
     ]
    }
   ],
   "source": [
    "#Q3 (c)\n",
    "\n",
    "#  X_train, X_val, y_train, y_val, X_test, y_test are already defined\n",
    "\n",
    "def train_and_evaluate(max_features, max_depth, min_samples_leaf):\n",
    "    dt = DecisionTreeClassifier(\n",
    "        max_features=max_features,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        random_state=42\n",
    "    )\n",
    "    dt.fit(X_train, y_train)\n",
    "    y_pred = dt.predict(X_val)\n",
    "    return accuracy_score(y_val, y_pred)\n",
    "\n",
    "# Define parameter ranges\n",
    "max_features_range = [None, 'sqrt', 'log2'] + list(range(1, X_train.shape[1] + 1))\n",
    "max_depth_range = [None] + list(range(1, 21))\n",
    "min_samples_leaf_range = range(1, 11)\n",
    "\n",
    "best_accuracy = 0\n",
    "best_params = {}\n",
    "\n",
    "# Use a triple for loop to iterate over different ranges for each of the three parameters\n",
    "for max_features in max_features_range:\n",
    "    for max_depth in max_depth_range:\n",
    "        for min_samples_leaf in min_samples_leaf_range:\n",
    "            accuracy = train_and_evaluate(max_features, max_depth, min_samples_leaf)\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy #find the best parameters\n",
    "                best_params = {\n",
    "                    'max_features': max_features,\n",
    "                    'max_depth': max_depth,\n",
    "                    'min_samples_leaf': min_samples_leaf\n",
    "                }\n",
    "                \n",
    "# Print those parameters out and then test them\n",
    "print(\"Best parameters:\", best_params)\n",
    "print(f\"Best validation accuracy: {best_accuracy:.4f}\")\n",
    "\n",
    "# Train to see if they are the best and then evaulate the test set\n",
    "final_dt = DecisionTreeClassifier(random_state=42, **best_params)\n",
    "final_dt.fit(X_train, y_train)\n",
    "y_pred_test = final_dt.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "print(f\"Test accuracy with best parameters: {test_accuracy:.4f}\")\n",
    "\n",
    "# Print confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_test)\n",
    "print(\"Confusion Matrix for the final model:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4\n",
    "\n",
    "Let's move onto random forests, we'll be doing more parameter tuning here.\n",
    "\n",
    "(a) With the original train and test sets, run a Random Forest model on the data and report the accuracy score. How does it compare to the scores in Question 3?\n",
    "\n",
    "(b) Create the train and validation sets again, and create a Random Forest Classifier with the following parameters: **n_estimators, max_leaf_nodes, max_depth**, with some values. What do this parameters do? (Again look up the documentation) Run the model and see how the accuracy changes. Change the values and try to get a higher accuracy.\n",
    "\n",
    "(c) Similar to Question 3 part (c), use a triple for loop to iterate over combinations of parameter values for the random forest and find one that is optimal in accuracy. How does this accuracy compare to the others we have seen?\n",
    "\n",
    "Note: This kind of parameter opimization can be done using built in python functions, GridSearchCV and RandomSearchCV both are methods that take in some kind of range / distribution for the parameters and finds the best one (and uses cross validation which is a bonus)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Result:\n",
      "\n",
      "accuracy score: 0.86\n",
      "\n",
      "Confusion Matrix: \n",
      " [[58  6]\n",
      " [ 8 28]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Q4 (a)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Use the same data as before\n",
    "X_train = titanic_train_data.drop(['Unnamed: 0', 'PassengerId', 'Survived'], axis=1)\n",
    "y_train = titanic_train_data['Survived']\n",
    "X_test = titanic_test_data.drop(['Unnamed: 0', 'PassengerId', 'Survived'], axis=1)\n",
    "y_test = titanic_test_data['Survived']\n",
    "\n",
    "# Fitting the random forest model using RandomForestClassifier\n",
    "rf_clf = RandomForestClassifier(n_estimators=100)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# report the accuracy score\n",
    "print_score(rf_clf, X_train, y_train, X_test, y_test, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy score between the decision tree and the random forest is similar, with the random forest having a higher accuracy rate being around .05 higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators=100, max_leaf_nodes=50, max_depth=10, max_features=sqrt\n",
      "Validation Accuracy: 0.8268\n",
      "\n",
      "n_estimators=200, max_leaf_nodes=None, max_depth=15, max_features=log2\n",
      "Validation Accuracy: 0.8268\n",
      "\n",
      "n_estimators=150, max_leaf_nodes=100, max_depth=None, max_features=None\n",
      "Validation Accuracy: 0.8346\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Q4 (b)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the training data into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "def train_and_evaluate_rf(n_estimators, max_leaf_nodes, max_depth, max_features):\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_leaf_nodes=max_leaf_nodes,\n",
    "        max_depth=max_depth,\n",
    "        max_features=max_features,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        random_state=42\n",
    "    )\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_pred = rf.predict(X_val)\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    return accuracy\n",
    "\n",
    "# Test different parameter combinations\n",
    "param_combinations = [\n",
    "    (100, 50, 10, 'sqrt', 2),\n",
    "    (200, None, 15, 'log2', 5),\n",
    "    (150, 100, None, None, 3)\n",
    "]\n",
    "\n",
    "for n_estimators, max_leaf_nodes, max_depth, max_features, min_samples_leaf in param_combinations:\n",
    "    accuracy = train_and_evaluate_rf(n_estimators, max_leaf_nodes, max_depth, max_features)\n",
    "    print(f\"n_estimators={n_estimators}, max_leaf_nodes={max_leaf_nodes}, max_depth={max_depth}, max_features={max_features}\")\n",
    "    print(f\"Validation Accuracy: {accuracy:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search Results:\n",
      "Best Parameters: {'n_estimators': 50, 'max_depth': 15, 'max_leaf_nodes': 2}\n",
      "Best Test Accuracy: 0.8402\n",
      "\n",
      "GridSearchCV Results:\n",
      "Best Parameters: {'max_depth': 5, 'max_leaf_nodes': 2, 'n_estimators': 50}\n",
      "Best Cross-Validation Accuracy: 0.8513\n",
      "Test Set Accuracy: 0.8333\n"
     ]
    }
   ],
   "source": [
    "#Q4 (c)\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define parameters using the same ranges as befpre\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [5, 10, 15, None],\n",
    "    'max_leaf_nodes': [2, 5, 10]\n",
    "}\n",
    "\n",
    "\n",
    "# Use a triple for loop to iterate over combinations of parameter values for the random forest and find one that is optimal in accuracy\n",
    "# Same as before\n",
    "best_accuracy = 0\n",
    "best_params = {}\n",
    "\n",
    "for n_est in param_grid['n_estimators']:\n",
    "    for depth in param_grid['max_depth']:\n",
    "        for min_samples in param_grid['max_leaf_nodes']:\n",
    "            rf = RandomForestClassifier(\n",
    "                n_estimators=n_est,\n",
    "                max_depth=depth,\n",
    "                min_samples_split=min_samples,\n",
    "                random_state=42\n",
    "            )\n",
    "            rf.fit(X_train, y_train)\n",
    "            accuracy = accuracy_score(y_test, rf.predict(X_test))\n",
    "            \n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_params = {\n",
    "                    'n_estimators': n_est,\n",
    "                    'max_depth': depth,\n",
    "                    'max_leaf_nodes': min_samples\n",
    "                }\n",
    "\n",
    "print(\"Grid Search Results:\")\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"Best Test Accuracy: {best_accuracy:.4f}\")\n",
    "\n",
    "# Using the GridSearchCV methods\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,  \n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1  \n",
    ")\n",
    "\n",
    "# Fit the grid search\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and accuracyscore\n",
    "print(\"\\nGridSearchCV Results:\")\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best Cross-Validation Accuracy: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Get the grid search accruacy\n",
    "grid_search_accuracy = accuracy_score(y_test, grid_search.predict(X_test))\n",
    "print(f\"Test Set Accuracy: {grid_search_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both methods found similar solutions, with there being only slight difference, which proves that GridSearchCV didn't significantly outperform the manual search on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus\n",
    "\n",
    "Come up with an analogy for decision tree's v. random forest's and why random forests avoid the problem of overfitting. (+5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An analogy for decision tree's v. random forest's is figuring out which areas of a forest could be at risk of deforestation. A single ranger could use perosnal rules due to years of experiences to assess risk, but that could also lead to the assesment being overly biased due to the specific experiences that the ranger had. If you create a larger team to asses the forest, such as scientists,rangers, community members, each will provide varied expertise and perspectives. By combining their assessments, the team creates a more balanced and accurate evaluation of forest vulnerability. This is similar to how a random forest model, which consists of multiple decision trees trained on different subsets of data, reduces the risk of overfitting by averaging the predictions of many diverse trees rather than relying on the potentially biased judgment of a single tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
